import re
import json
import time
from typing import List, Dict, Tuple
from datetime import datetime
from tqdm import tqdm

from .utils import (
    get_candidate_repos, get_repository_info, has_test_cases,
    get_repository_readme, get_ci_configs, extract_version_components,
    get_repository_releases, Release, Repository
)
from .config import (
    CRAWL_MODE, CRAWL_JSON_FILE, MIN_STARS, RANK_START, RANK_END,
    MIN_RELEASES, MIN_RELEASE_BODY_LENGTH, MIN_RELEASE_DATE, EXCLUDED_TOPICS,
    TEST_DIRECTORIES, TEST_FILE_PATTERNS, BOT_USERS, CACHE_FILE,
    DEFAULT_RELEASE_LIMIT
)

# --- Cache Management Functions ---

def load_processed_repos() -> Dict[str, Repository]:
    """Load processed repository information from JSON file."""
    if CACHE_FILE.exists():
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                processed_repos = {}
                for repo_name, repo_data in data.items():
                    processed_repos[repo_name] = Repository.from_dict(repo_data)
                print(f"‚úÖ Loaded {len(processed_repos)} processed repositories from cache file")
                return processed_repos
        except (json.JSONDecodeError, Exception) as e:
            print(f"‚ö†Ô∏è Failed to load cache file: {e}, will restart processing")
            return {}
    else:
        print("üìù Cache file does not exist, will create new cache")
        return {}

def save_processed_repo(repository: Repository):
    """Save processing result of a single repository to JSON file."""
    # Load existing data
    processed_repos_dict = {}
    if CACHE_FILE.exists():
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                processed_repos_dict = json.load(f)
        except:
            pass
    
    # Add new repository data
    processed_repos_dict[repository.full_name] = repository.to_dict()
    
    # Save to file
    try:
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(processed_repos_dict, f, indent=2, ensure_ascii=False)
        print(f"üíæ Saved processing result for repository {repository.full_name} to cache")
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to save cache: {e}")

def is_valid_release(release_data: dict) -> bool:
    """Check if release is valid (not generated by bot and content is substantial and after specified date)"""
    # Check if generated by bot
    author_login = release_data.get('author', {}).get('login', '')
    if author_login in BOT_USERS:
        return False
    
    # Check release body length
    body = release_data.get('body', '') or ''  # Ensure body is not None
    if len(body.strip()) < MIN_RELEASE_BODY_LENGTH:
        return False
    
    # Check if publish time is after specified date
    published_at = release_data.get('published_at', '')
    if published_at:
        try:
            release_date = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
            min_date = datetime.fromisoformat(MIN_RELEASE_DATE + 'T00:00:00+00:00')
            if release_date < min_date:
                return False
        except Exception:
            # If date parsing fails, skip this release
            return False
    else:
        # If no publish date, skip this release
        return False
    
    return True

def filter_by_metadata_and_releases(repos) -> List[Dict]:
    """Preliminary filtering of repositories by macro indicators and release count via API."""
    print(f"Preliminary filtering by macro indicators and release count (ranking range {RANK_START}-{RANK_END})...")
    
    filtered_repos = []
    
    # Use tqdm to show filtering progress
    with tqdm(repos, desc="Filtering repositories", unit="repo") as pbar:
        for repo in pbar:
            repo_name = repo['full_name']
            repo_rank = repo.get('rank', 0)
            pbar.set_description(f"Checking: {repo_name} (rank#{repo_rank})")

            # 1. Check macro indicators (topics filtering)
            repo_topics = set(repo.get('topics', []))
            if repo_topics.intersection(EXCLUDED_TOPICS):
                pbar.write(f"  ‚ùå {repo_name} (#{repo_rank}): Contains excluded topics")
                continue

            # 2. Check if has test cases
            if not has_test_cases(repo_name, TEST_DIRECTORIES, TEST_FILE_PATTERNS):
                pbar.write(f"  ‚ùå {repo_name} (#{repo_rank}): No test cases")
                continue

            # 3. Check if has enough valid releases
            try:
                releases = get_repository_releases(repo_name)

                # Filter valid releases
                valid_releases = [r for r in releases if is_valid_release(r)]

                if len(valid_releases) >= MIN_RELEASES:
                    repo['releases_count'] = len(valid_releases)
                    repo['releases_data'] = valid_releases  # Save filtered releases data
                    filtered_repos.append(repo)
                    pbar.write(f"  ‚úÖ {repo_name} (#{repo_rank}): Passed initial screening! Stars: {repo['stargazers_count']}, Valid releases: {len(valid_releases)}")
                else:
                    pbar.write(f"  ‚ùå {repo_name} (#{repo_rank}): Insufficient valid releases, only {len(valid_releases)}")

            except Exception as e:
                pbar.write(f"  ‚ùå {repo_name} (#{repo_rank}): API Error: {str(e)}")
    
    return filtered_repos

def get_major_releases(repo_full_name: str, releases_data, limit=5) -> List[Release]:
    """Get major version releases for repository (group by major version then take latest from each group)."""
    print(f"  > Getting major version releases for {repo_full_name}...")
    
    all_releases = releases_data
    print(f"  > Using {len(all_releases)} valid releases data already fetched")
    
    valid_releases = []
    
    for release in all_releases:
        tag_name = release.get('tag_name', '')
        
        # Use new version extraction function
        version_tuple = extract_version_components(tag_name)
        
        if version_tuple:
            # Skip pre-release versions (containing alpha, beta, rc, a, b identifiers)
            if re.search(r'(alpha|beta|rc|a\d+|b\d+)', tag_name.lower()):
                continue
            
            release_obj = Release(
                tag_name=tag_name,
                name=release.get('name', ''),
                body=release.get('body', ''),
                published_at=release.get('published_at', ''),
                target_commitish=release.get('target_commitish', ''),
                version_tuple=version_tuple,
                version_key='.'.join(str(v) for v in version_tuple),
            )
            valid_releases.append(release_obj)
    
    # Sort by version number, take latest few versions
    valid_releases.sort(key=lambda x: x.version_tuple, reverse=True)
    result = valid_releases[:limit]  # Take only first few versions
    
    print(f"  > Successfully got {len(result)} major version releases")
    if result:
        version_list = ', '.join([r.version_key for r in result])
        print(f"  > Selected versions: {version_list}")
    return result

def process_single_repository(repo: Dict, use_cache: bool = True) -> Repository:
    """Process single repository, get its details"""
    repo_name = repo['full_name']
    
    # Get major version releases, using limit from config
    major_releases = get_major_releases(
        repo_name,
        releases_data=repo.get('releases_data'),
        limit=DEFAULT_RELEASE_LIMIT
    )
    if not major_releases:
        raise ValueError(f"Unable to get major version releases")
        
    # Get README content
    readme_content = get_repository_readme(repo_name)

    # Get CI/CD configurations
    ci_configs = get_ci_configs(repo_name)
    
    # Create Repository object
    repository = Repository(
        full_name=repo_name,
        stargazers_count=repo['stargazers_count'],
        size=repo['size'],
        topics=repo.get('topics', []),
        releases_count=repo['releases_count'],
        major_releases=major_releases,
        readme_content=readme_content,
        ci_configs=ci_configs,
        processed_at=time.strftime('%Y-%m-%d %H:%M:%S')
    )
    
    # Save to cache
    if use_cache:
        save_processed_repo(repository)
    
    return repository

def get_specified_repos() -> List[Dict]:
    """Get specified repository list from crawl.json file"""
    print(f"Getting repository list from specified file: {CRAWL_JSON_FILE}")
    
    if not CRAWL_JSON_FILE.exists():
        print(f"‚ùå Specified repository file does not exist: {CRAWL_JSON_FILE}")
        return []
    
    try:
        with open(CRAWL_JSON_FILE, 'r', encoding='utf-8') as f:
            crawl_data = json.load(f)
        
        # Collect repositories from all categories
        all_repos = []
        for category, repos in crawl_data.items():
            print(f"‚úÖ Loaded category '{category}': {len(repos)} repositories")
            for repo_name in repos:
                all_repos.append(repo_name)
        
        print(f"‚úÖ Total loaded {len(all_repos)} specified repositories")
        
        # Get details for each repository
        detailed_repos = []
        with tqdm(all_repos, desc="Getting repository info", unit="repo") as pbar:
            for repo_name in pbar:
                pbar.set_description(f"Getting: {repo_name}")
                try:
                    repo_info = get_repository_info(repo_name)
                    if repo_info:
                        detailed_repos.append(repo_info)
                        pbar.write(f"  ‚úÖ {repo_name}: Stars {repo_info['stargazers_count']}")
                    else:
                        pbar.write(f"  ‚ùå {repo_name}: Failed to get info")
                except Exception as e:
                    pbar.write(f"  ‚ùå {repo_name}: {str(e)}")
                    continue
                
                time.sleep(0.5)  # Avoid API limit
        
        print(f"‚úÖ Successfully got details for {len(detailed_repos)} repositories")
        return detailed_repos
        
    except Exception as e:
        print(f"‚ùå Failed to read specified repository file: {e}")
        return []

def get_repositories_to_process(use_cache: bool = True) -> Tuple[List[Dict], Dict[str, Repository]]:
    """Get list of repositories to process and processed repositories"""
    # Load processed repository cache
    processed_repos = load_processed_repos() if use_cache else {}
    
    # Get candidate repositories based on mode
    if CRAWL_MODE == "specified":
        print("üéØ Using specified repository mode")
        candidate_repos = get_specified_repos()
    else:
        print("‚≠ê Using star count filtering mode")
        candidate_repos = get_candidate_repos(MIN_STARS, RANK_START, RANK_END)
    
    if not candidate_repos:
        return [], processed_repos

    # Filter out already processed repositories
    if processed_repos:
        unprocessed_repos = [repo for repo in candidate_repos if repo['full_name'] not in processed_repos]
        candidate_repos = unprocessed_repos

    # Preliminary filtering
    pre_filtered_repos = filter_by_metadata_and_releases(candidate_repos)
    
    return pre_filtered_repos, processed_repos