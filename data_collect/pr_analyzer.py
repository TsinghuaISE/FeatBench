import ast
import json
import time
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass, asdict
import openai
from tqdm import tqdm

from .utils import is_test_file, get_pr_info, get_pr_files, get_file_content, get_commit_info, extract_pr_number_from_url, FileChange, Commit
from .config import (
    OPENAI_API_KEY, OPENAI_MODEL, OPENAI_BASE_URL, PR_ANALYSIS_CACHE_FILE,
    MAX_FILES_IN_SUMMARY, MAX_PATCH_LENGTH, MAX_PATCH_PREVIEW_LENGTH, PROMPTS
)

# --- Data Class Definitions ---

@dataclass
class TestFile:
    """Represents a test file"""
    path: str
    content: str
    size: int

    def to_dict(self) -> Dict:
        return asdict(self)

    @classmethod
    def from_dict(cls, data: Dict) -> 'TestFile':
        return cls(**data)

@dataclass
class PRAnalysis:
    """Represents detailed analysis result of a PR"""
    pr_number: str
    repo_name: str
    title: str
    description: str
    state: str  # 'open', 'closed', 'merged'
    merged: bool
    base_commit: Commit  # commit info before PR
    head_commit: Commit  # commit info after PR
    file_changes: List[FileChange]
    detailed_description: str  # detailed description generated by LLM based on file changes
    has_tests: bool  # whether related tests were found
    test_files: List[str]  # test file path list
    only_modified_existing_functions: bool # whether only existing functions were modified
    non_test_files: List[str] # non-test file path list
    analyzed_at: str
    
    def to_dict(self) -> Dict:
        return {
            'pr_number': self.pr_number,
            'repo_name': self.repo_name,
            'title': self.title,
            'description': self.description,
            'state': self.state,
            'merged': self.merged,
            'base_commit': self.base_commit.to_dict(),
            'head_commit': self.head_commit.to_dict(),
            'file_changes': [fc.to_dict() for fc in self.file_changes],
            'detailed_description': self.detailed_description,
            'has_tests': self.has_tests,
            'test_files': self.test_files,
            'only_modified_existing_functions': self.only_modified_existing_functions,
            'non_test_files': self.non_test_files or [],
            'analyzed_at': self.analyzed_at
        }
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'PRAnalysis':
        return cls(
            pr_number=data['pr_number'],
            repo_name=data['repo_name'],
            title=data['title'],
            description=data['description'],
            state=data['state'],
            merged=data['merged'],
            base_commit=Commit.from_dict(data.get('base_commit', {})),
            head_commit=Commit.from_dict(data.get('head_commit', {})),
            file_changes=[FileChange.from_dict(fc) for fc in data.get('file_changes', [])],
            detailed_description=data.get('detailed_description', ''),
            has_tests=data.get('has_tests', False),
            test_files=data.get('test_files', []),
            only_modified_existing_functions=data.get('only_modified_existing_functions', True),
            non_test_files=data.get('non_test_files', []),
            analyzed_at=data.get('analyzed_at', '')
        )

@dataclass
class EnhancedFeature:
    """Enhanced feature object containing PR detailed analysis"""
    feature_type: str
    description: str
    pr_analyses: List[PRAnalysis]
    feature_detailed_description: str  # overall detailed description based on all PR analyses
    
    def to_dict(self) -> Dict:
        return {
            'feature_type': self.feature_type,
            'description': self.description,
            'pr_analyses': [pr.to_dict() for pr in self.pr_analyses],
            'feature_detailed_description': self.feature_detailed_description
        }
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'EnhancedFeature':
        return cls(
            feature_type=data['feature_type'],
            description=data['description'],
            pr_analyses=[PRAnalysis.from_dict(pr) for pr in data.get('pr_analyses', [])],
            feature_detailed_description=data.get('feature_detailed_description', '')
        )

# --- Cache Management ---

def load_pr_analysis_cache() -> Dict[str, PRAnalysis]:
    """Load PR analysis cache"""
    if PR_ANALYSIS_CACHE_FILE.exists():
        try:
            with open(PR_ANALYSIS_CACHE_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                cache = {}
                for key, pr_data in data.items():
                    cache[key] = PRAnalysis.from_dict(pr_data)
                print(f"‚úÖ Loaded {len(cache)} PR analysis results from cache")
                return cache
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to load PR analysis cache: {e}")
            return {}
    return {}

def save_pr_analysis_to_cache(analysis: PRAnalysis):
    """Save PR analysis result to cache"""
    cache = {}
    if PR_ANALYSIS_CACHE_FILE.exists():
        try:
            with open(PR_ANALYSIS_CACHE_FILE, 'r', encoding='utf-8') as f:
                cache = json.load(f)
        except:
            pass
    
    cache_key = f"{analysis.repo_name}#{analysis.pr_number}"
    cache[cache_key] = analysis.to_dict()
    
    try:
        with open(PR_ANALYSIS_CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache, f, indent=2, ensure_ascii=False)
        print(f"üíæ Saved analysis result for PR#{analysis.pr_number} to cache")
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to save PR analysis cache: {e}")

def extract_definitions(content: str) -> List[str]:
    """Extract function and class definitions from Python code content, including nested relationships"""
    if not content:
        return []
    
    try:
        # Parse code into AST
        tree = ast.parse(content)
        
        # Store all definitions (including their paths)
        definitions = []
        
        def visit_node(node, path=""):
            """Recursively visit nodes and collect definition information"""
            if isinstance(node, ast.FunctionDef):
                full_name = f"{path}.{node.name}" if path else node.name
                definitions.append(full_name)
                
                # Recursively visit definitions within function body
                for child in node.body:
                    visit_node(child, full_name)
                    
            elif isinstance(node, ast.ClassDef):
                full_name = f"{path}.{node.name}" if path else node.name
                definitions.append(full_name)
                
                # Recursively visit definitions within class body
                for child in node.body:
                    visit_node(child, full_name)
            
            elif isinstance(node, ast.Module):
                # Top-level node of module
                for child in node.body:
                    visit_node(child)
        
        # Start visiting
        visit_node(tree)
        return definitions
        
    except SyntaxError:
        print(f"    - ‚ö†Ô∏è Code parsing error, may contain syntax errors")
        return []

def analyze_function_changes(before_content: str, after_content: str) -> Tuple[bool, List[str], List[str]]:
    """Analyze function changes, return whether only existing functions were modified and lists of added/deleted functions"""
    
    # Analyze file content changes (using full paths to distinguish nested relationships)
    before_definitions = set(extract_definitions(before_content or ""))
    after_definitions = set(extract_definitions(after_content or ""))
    
    # Find added and deleted definitions (including functions and classes)
    new_definitions = list(after_definitions - before_definitions)
    deleted_definitions = list(before_definitions - after_definitions)
    
    # Determine if only existing functions and classes were modified
    only_modified = len(new_definitions) == 0 and len(deleted_definitions) == 0
    
    return only_modified, new_definitions, deleted_definitions

# --- LLM Analysis ---

def generate_detailed_description_with_llm(
    feature_description: str,
    pr_info: Dict,
    file_changes: List[FileChange]
) -> Optional[str]:
    """Use LLM to generate detailed feature description based on file changes"""

    client = openai.OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)

    # Filter out test files, only build summary for non-test files
    non_test_file_changes = [fc for fc in file_changes if not is_test_file(fc.filename)]

    # Build file change summary
    files_summary = []
    for fc in non_test_file_changes[:MAX_FILES_IN_SUMMARY]:  # Use config file count limit
        summary = f"- {fc.filename} ({fc.status}): +{fc.additions}/-{fc.deletions}"
        if fc.patch and len(fc.patch) < MAX_PATCH_LENGTH:  # Use config patch length limit
            summary += f"\n  {fc.patch[:MAX_PATCH_PREVIEW_LENGTH]}..."  # Use config preview length
        files_summary.append(summary)

    files_text = "\n".join(files_summary)
    if len(non_test_file_changes) > MAX_FILES_IN_SUMMARY:
        files_text += f"\n... and {len(non_test_file_changes) - MAX_FILES_IN_SUMMARY} more files"

    # If no non-test files, provide prompt info
    if not non_test_file_changes:
        files_text = "No files were modified in this PR."

    # Get prompts from config
    pr_analysis_system_prompt = PROMPTS.pr_analysis_system
    pr_analysis_user_prompt_template = PROMPTS.pr_analysis_user

    # Format the user prompt with variables
    prompt = pr_analysis_user_prompt_template.format(
        feature_description=feature_description,
        pr_title=pr_info.get('title', ''),
        pr_body=pr_info.get('body', ''),
        files_text=files_text
    )

    try:
        response = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": pr_analysis_system_prompt},
                {"role": "user", "content": prompt}
            ],
            temperature=0,
            max_tokens=8192
        )

        content = response.choices[0].message.content
        return content if content is not None else feature_description
        
    except Exception as e:
        print(f"‚ö†Ô∏è LLM failed to generate detailed description: {e}")
        return feature_description

def generate_feature_detailed_description(
    feature_description: str,
    feature_type: str,
    pr_analyses: List[PRAnalysis]
) -> Optional[str]:
    """Generate detailed description for the entire feature based on detailed analyses of multiple PRs"""

    client = openai.OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)

    # Build detailed summary of all PRs
    pr_summaries = []
    for pr in pr_analyses:
        summary = f"""
PR #{pr.pr_number}: {pr.title}
- Status: {pr.state} (merged: {pr.merged})
- Files changed: {len(pr.file_changes)}
- User benefits: {pr.detailed_description}
"""
        pr_summaries.append(summary)
    
    prs_text = "\n".join(pr_summaries)

    # Get prompts from config
    feature_analysis_system_prompt = PROMPTS.feature_analysis_system
    feature_analysis_user_prompt_template = PROMPTS.feature_analysis_user

    # Format the user prompt with variables
    prompt = feature_analysis_user_prompt_template.format(
        feature_type=feature_type,
        feature_description=feature_description,
        prs_text=prs_text
    )

    try:
        response = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": feature_analysis_system_prompt},
                {"role": "user", "content": prompt}
            ],
            temperature=0,
            max_tokens=1000
        )
        
        content = response.choices[0].message.content
        return content if content is not None else feature_description
        
    except Exception as e:
        print(f"‚ö†Ô∏è LLM failed to generate feature detailed description: {e}")
        return None

# --- Main Function ---

def analyze_pr(repo_name: str, pr_url: str, feature_description: str, use_cache: bool = True) -> Optional[PRAnalysis]:
    """Analyze a single PR"""
    pr_number = extract_pr_number_from_url(pr_url)
    if not pr_number:
        print(f"‚ö†Ô∏è Unable to extract PR number from URL: {pr_url}")
        return None
    
    cache_key = f"{repo_name}#{pr_number}"
    
    # Check cache
    if use_cache:
        cache = load_pr_analysis_cache()
        if cache_key in cache:
            print(f"  > üîÑ Loading PR#{pr_number} analysis result from cache")
            return cache[cache_key]
    
    print(f"  > üîç Analyzing PR#{pr_number}...")
    
    # Get PR basic info
    pr_info = get_pr_info(repo_name, pr_number)
    if not pr_info:
        return None
    
    # Get file changes
    file_changes = get_pr_files(repo_name, pr_number)
    
    # Get detailed commit info
    base_sha = pr_info['base']['sha']
    head_sha = pr_info['head']['sha']
    
    base_commit = get_commit_info(repo_name, base_sha)
    head_commit = get_commit_info(repo_name, head_sha)
    
    # If unable to get commit info, create basic Commit objects
    if not base_commit:
        base_commit = Commit(sha=base_sha, message='', date='', author='')
    if not head_commit:
        head_commit = Commit(sha=head_sha, message='', date='', author='')
    
    # Get detailed commit info
    base_commit = get_commit_info(repo_name, base_sha)
    head_commit = get_commit_info(repo_name, head_sha)

    # If unable to get commit info, create basic Commit objects
    if not base_commit:
        base_commit = Commit(sha=base_sha, message='', date='', author='')
    if not head_commit:
        head_commit = Commit(sha=head_sha, message='', date='', author='')

    test_files = []
    non_test_files = []
    detailed_description = None
    only_modified_existing_functions = True

    for file_data in file_changes:
        file_path = file_data.filename
        
        if is_test_file(file_path):
            test_files.append(file_path)
            print(f"    - Found test file: {file_path}")
        elif file_path.endswith('.py'):  # Only analyze Python files
            non_test_files.append(file_path)
            
            # Check function changes
            status = file_data.status
            
            # For added or removed files, as long as they contain function definitions, condition not met
            if status == 'added' or status == 'removed':
                content = get_file_content(repo_name, file_path, head_commit.sha if status == 'added' else base_commit.sha)
                if content and extract_definitions(content):
                    print(f"    - ‚ö†Ô∏è Found {status} file containing function definitions: {file_path}")
                    only_modified_existing_functions = False
                    break
            
            # For modified files, need to compare function definitions before and after modification
            elif status == 'modified':
                before_content = get_file_content(repo_name, file_path, base_commit.sha)
                after_content = get_file_content(repo_name, file_path, head_commit.sha)

                # Handle case where file content might be None
                if before_content is not None and after_content is not None:
                    only_modified, new_funcs, deleted_funcs = analyze_function_changes(before_content, after_content)
                
                    if not only_modified:
                        print(f"    - ‚ö†Ô∏è File modification contains function additions or deletions: {file_path}")
                        if new_funcs:
                            print(f"      Added functions: {', '.join(new_funcs)}")
                        if deleted_funcs:
                            print(f"      Deleted functions: {', '.join(deleted_funcs)}")
                        only_modified_existing_functions = False
                        break
                else:
                    print(f"    - ‚ö†Ô∏è Unable to get file content, may be empty or non-existent: {file_path}")
                    only_modified_existing_functions = False
    
    # Generate detailed description
    if only_modified_existing_functions and test_files and non_test_files:
        detailed_description = generate_detailed_description_with_llm(
            feature_description, pr_info, file_changes
        )
    
    if not detailed_description:
        if not test_files:
            print(f"  > ‚è≠Ô∏è PR#{pr_number} does not contain test file changes, skipping analysis")
        elif not non_test_files:
            print(f"  > ‚è≠Ô∏è PR#{pr_number} does not contain non-test file changes, skipping analysis")
        elif not only_modified_existing_functions:
            print(f"  > ‚è≠Ô∏è PR#{pr_number} contains function additions or deletions, skipping analysis")
        return None
    
    analysis = PRAnalysis(
        pr_number=pr_number,
        repo_name=repo_name,
        title=pr_info.get('title', ''),
        description=pr_info.get('body', ''),
        state=pr_info.get('state', ''),
        merged=pr_info.get('merged', False),
        base_commit=base_commit,
        head_commit=head_commit,
        file_changes=file_changes,
        detailed_description=detailed_description,
        has_tests=len(test_files) > 0,
        test_files=test_files,
        only_modified_existing_functions=only_modified_existing_functions,
        non_test_files=non_test_files,
        analyzed_at=time.strftime('%Y-%m-%d %H:%M:%S')
    )

    print(f"    - PR#{pr_number}: has_tests={analysis.has_tests}, test_file_count={len(analysis.test_files)}, only_modified_functions={analysis.only_modified_existing_functions}, only_modified_existing={analysis.only_modified_existing_functions}")

    # Save to cache
    if use_cache:
        save_pr_analysis_to_cache(analysis)
    
    return analysis

def enhance_feature_with_pr_analysis(feature, repo_name: str) -> Optional[EnhancedFeature]:
    """Enhance feature object by adding PR detailed analysis"""
    pr_analyses = []
    
    # Use tqdm to show PR analysis progress
    with tqdm(feature.pr_links, desc=f"Analyzing PRs", unit="pr", leave=False) as pbar:
        for pr_link in pbar:
            pr_number = extract_pr_number_from_url(pr_link)
            pbar.set_description(f"PR#{pr_number}")
            
            pr_analysis = analyze_pr(repo_name, pr_link, feature.description)
            if pr_analysis:
                pr_analyses.append(pr_analysis)
            
            # Avoid API rate limit
            time.sleep(0.5)
    
    # If only one PR, use PR's detailed description directly
    if len(pr_analyses) == 1:
        feature_detailed_description = pr_analyses[0].detailed_description
    elif len(pr_analyses) > 1:
        # For multiple PRs, generate feature detailed description based on all PR analyses
        feature_detailed_description = generate_feature_detailed_description(
            feature.description,
            feature.feature_type,
            pr_analyses
        )
    else:
        return None
    
    if feature_detailed_description:
        return EnhancedFeature(
            feature_type=feature.feature_type,
            description=feature.description,
            pr_analyses=pr_analyses,
            feature_detailed_description=feature_detailed_description
        )
    else:
        return None

def enhance_release_analysis_with_pr_details(release_analysis) -> List[EnhancedFeature]:
    """Enhance release analysis, only process new_features to add PR details"""
    print(f"--- Starting analysis of new features for {release_analysis.tag_name} ---")
    
    enhanced_features = []
    
    # Use tqdm to show feature analysis progress
    with tqdm(release_analysis.new_features, desc=f"Analyzing features", unit="feature", leave=False) as pbar:
        for feature in pbar:
            if feature.pr_links:
                pbar.set_description(f"Feature: {feature.description[:30]}...")
                enhanced = enhance_feature_with_pr_analysis(feature, release_analysis.repo_name)
                if enhanced:
                    enhanced_features.append(enhanced)
                    pbar.write(f"    ‚úÖ Analyzed feature: {feature.description[:50]}...")
                else:
                    pbar.write(f"    ‚ö†Ô∏è Skipped feature: {feature.description[:50]}...")
    
    return enhanced_features